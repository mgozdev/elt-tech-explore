# dlt Secrets Configuration Template
# ===================================
#
# Copy this file to secrets.toml and fill in your credentials
# DO NOT commit secrets.toml to git (it's gitignored)
#
# See DATABRICKS_SETUP.md for instructions on getting these values

# =============================================================================
# DATABRICKS CREDENTIALS
# =============================================================================

[destination.databricks.credentials]
# Your Databricks workspace hostname (without https://)
# Example: "adb-1234567890123456.7.azuredatabricks.net"
# Find it in: Workspace URL (remove https://)
server_hostname = "<your-databricks-workspace>.azuredatabricks.net"

# SQL Warehouse HTTP path
# Example: "/sql/1.0/warehouses/abc123def456"
# Find it in: SQL Warehouses > Your Warehouse > Connection Details
http_path = "/sql/1.0/warehouses/<your-warehouse-id>"

# Personal Access Token
# Example: "dapi1234567890abcdef..."
# Create it in: User Settings > Developer > Access Tokens
access_token = "<your-databricks-token>"

# Catalog name (optional, defaults to "main")
# Unity Catalog name where data will be stored
catalog = "main"

# =============================================================================
# DUCKLAKE CREDENTIALS
# =============================================================================

[destination.ducklake.credentials]
ducklake_name = "ducklake"

# PostgreSQL catalog for table metadata
catalog = "postgres://<username>:<password>@<host>:<port>/<database>"

# MinIO storage (S3-compatible)
# Note: Uses AWS SDK naming conventions because MinIO is S3-compatible
# IMPORTANT:
#   - bucket_url MUST use s3:// scheme even for MinIO
#   - Must include a subfolder (e.g., s3://bucket/data) - cannot be just the bucket root
#   - The subfolder becomes the DuckLake DATA_PATH root
# The endpoint_url parameter tells fsspec to use MinIO instead of AWS
[destination.ducklake.credentials.storage]
bucket_url = "s3://<bucket-name>/data"

[destination.ducklake.credentials.storage.credentials]
# These are passed to fsspec for file operations
aws_access_key_id = "<your-minio-access-key>"
aws_secret_access_key = "<your-minio-secret-key>"
endpoint_url = "http://<minio-host>:<port>"
region_name = "us-east-1"

# DuckDB global config for S3 access (separate from fsspec above)
# These settings configure DuckDB's internal S3 client
[destination.ducklake.credentials.global_config]
s3_endpoint = "<minio-host>:<port>"
s3_access_key_id = "<your-minio-access-key>"
s3_secret_access_key = "<your-minio-secret-key>"
s3_url_style = "path"
s3_use_ssl = "false"
s3_region = "us-east-1"

# =============================================================================
# OTHER DESTINATIONS (for future use)
# =============================================================================

# [destination.postgres.credentials]
# database = "nport"
# username = "<your-username>"
# password = "<your-password>"
# host = "localhost"
# port = 5432

# [destination.bigquery.credentials]
# project_id = "<your-project-id>"
# # Use service account key file or default credentials
